# Lecture 15
Charles J. Antonelli
10-31-2016

### A Flux compute node

* 12-24 Intel cores
* 4 GB/core
    * Kernel space (system files)
    * User space (contain "process" files)
* 48-128 GB RAM
* Local disk
* Network

### Parallel programming models

* Data parllelism (SIMD)
    * Early supercomuting work done here - vecotr machines (Illiac IV, Cray-1)
    * Survives in, e.g., Intel AVX-512, Xeon Phi 512-bit ISA
    * GPGPUs fit in here as well
* Shared memory
* Message passing
* Remote meory operations (RDMA, Remote Direct Memory Access)
    * InfiniBand

### Parallelism models

* Message-passing: appication consists of several processes running on different nodes and communicating with each other over the network
    * Talk to other machines over **network** (*at least* 1000 times slower than memory access speed)
    * Used when
        * data are too large to fit on a single node
        * computational needs exceed a single node's resources
        * you can tolerate the communication overhead
    * AKA "Coarse parallelism" or "SPMD" (single program multiple data)
    * Implemented using MPI libraries
* Multi-threaded: application consists of a single process containing several parallel threads that communicate with each other using synchronization primitives
    * Used when
        * data can fit onto a single node
        * communications overhead of the M-P model is intolerable
    * AKA "Fine-grained para.." or "shared-mem para.."
    * Implemented using OpenMP compilers & libraries
* Hybrid: application consists of several processes running on differnt nodes and communicating with each other over the network, each of which contains several parallel threads that communicate with each other using synchronization primitives

### Cluster batch workflow

* You create a (bash) *batch script* and submit it to PBS
* PBS schedules your job, and it enters the flux queue

### Why message-passing

* Universality: fits well on separate processors connected by networks
* Expressivit: Useful and complete model for expressing parallel algorithms
* Performance: Allows programmer to explicitly associate data with processes while the compiler and cache management hardware manage efficient data access
    * memory-bound applications can exhibit superlinear speedup

### MPI Implementations

* MPICH: intended a high-quality reference implementation of the latest MPI standard
* Open MPI: Open-source MPI implementation, developed to target the common case
* MVAPICH: adds support for InfiniBand, Omni-Path, Ethernet/iWARP, and RoCE networking technologies (RDMA)

### MPI operations

* Two baic kinds of MPI operations
    * point-to-point
        * one sender, one receiver
    * collective
        * one sender, many receivers (MPI_Broadcast)
        * Many senders, many receivers (MPI_Alltoall)
        * Synchronization (MPI_Barrier)

### Advanced topic: one-sided communication (MPI 3.0)

* Define a "window", which can be accessed by remote ranks
    * MPI_WinCreate(..)
* Then use:
    * MPI_Put()
    * MPI_Get()
    * MPI_Accumulate()    

### Advanced topic

* Dynamic process management
    * MPI_Comm_spawn
* MPI-IO
    * API for abstracting parallel I/O    
* C++
    * Support has been deprecated