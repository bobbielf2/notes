# Lecture 13, Performance


## Recap of Architecture

### Idealized Processor vs Real world processors

* Real world processors: more complex
* "single" processor concept
* Memory hierarchy: smaller->faster, bigger->slower
* The SDSC Comet: 2 Sockets

## Performance Metrics

### Meaning of "fast" code

* The real metric: Time
* Derived metrics
    * FLOPS = floating point operations per sec
    * Bandwidth = data per unit time (sort of like a flow rate)
    * Latency = Minimum time for data to travel from point A to point B (time for sending 0-bit info).
* How to get fast code?
    * First: choose the *right algorithm*
    * Second: understand how to express that algorithm optimally in the *programming language*
    * Third: understand how source cdoe will get mapped to the *hardware*
    * Fourth: tune the code to the hardware
    * *A lot of this can be done with pen and paper*

### Peak performance

* Example: processor on Comet (Intel Haswell)
    * Maximum FLOPs per cycle?
        * Need to look at SID info on processor
        * If we have AVX it supports a 256-bit vector so it can operate on 4 doubles
    * Support a fused multiply add (FMA instruction)?
        * Yes, so the chip can execute 4 FMA instructions (8 FLOPs) at once
    * How many vector units does it have?
        * Apparently it has 2 vector units... so now we're at 16 FLOPs at once
    * How many cycles to execute an FMA instruction (which is two operations)?
        * Not always easy to find... common to assume 1 cycle
    * What's the clock speed (cycles per second)? 2.50 GHz
        * Well with AVX it appears to be 2.1 GHz
    * How many cores does it have? 12
* 16 FLOPs/cycle x 2.5e9 cycles/sec x 12 cores = 480 GFLOPS
    * 16 FLOPs/cycle / 5 cycle x 2.1e9 cycles/sec x 12 cores = 80 GFLOPS

### Algorithm Performance

* Not all algorithms are created equally
    * e.g. $O(n^2), O(n^3)$
* Not all algorithms are created equally
    * can have "same" implementations with vastly different performance
* Very few algorithms allow you to achieve *sustained performance* at a significant fraction of the *theoretical peak*
* Lets go through an example:



## Algorithm Performance: Example Mat-Vec Multiply

### Simple Performance Model

* 2 levels of mem in hierarchy: fast and slow
* model predict execution time for some set of operations: $T = Ft_F + t_M L$
* Min possible time is $Ft_F$ when all data is in fast memory
* Can also rewrite as $T = Ft_F(1+ \frac{t_M}{t_F} \frac{L}{F})$
    * $\frac{t_M}{t_F}$ is machine balance, key to machine efficiency, generally $t_F\ll t_M$
    * $\frac{L}{F}$ is computational efficiency, key to algoirthm efficiency.

### Slide 14
* $L = 3n$ (read write vectors) $+ n^2$ (read matrix)
* $F = 2n^2$ (arithmetic operations)
* $q = F/L = 2$

### Simplifying Assumptions

* ignored parallelism in processor between memory and arithmetic within the processor
* Assumed fast mem was large enough to hold three vectors
* Assumed the cost of a fast memory access is 0
* Memory latency is constant
* **So this analysis is "best case" scenario**

### Other performance models

* Latency based execution time model
* When dealing  with more complex kernels


page 18

* Do NOT optimize too prematurely
* quote 3: inf speed up (LOL)

## Common techniques for serial optimization

### Before you program
* Choose the best algorithm
    * fastest converging, asymptotically small operation counts, etc
* Choose best way to express this algo in a programming language
    * minimize memory traffic and communication

### As you are programming

* operation choice
    * addition over exp
    * multiply over division (maybe)
* Expose independent operations
    * use local variables to expose independent operations that can execute in parallel (or pipelined)
* Exploit multiple registers
    * reduce demands on memory bandwidth by pre-loading into local variables
* Loop unrolling 
    * compilers are not necessarily very good at interpreting how to *optimize loops*
        * provide the "right flags"
    * Exploit vector instructinos and pipelining
    * Cannot be done to arbitrary size
        * registers will be overloaded
        * size should be "register-blocked" or "cache-blocked"
* Remove conditionals
    * Loops with branching onstructs are usually *not* optimized by compiler (e.g., IF-clause inside FOR-loop)
* Removing false dependencies
    * using local variables, reorder operations to remove false dependencies
    * With some compilers, you can declare $a$ and $b$ unaliased
        * done via "restrict poiters", compiler flags
* Function tabulation
    * Some special functions (exp, log, gamma, error function, etc)
        * Require many FLOPs to evaluate to double precision
    * Tabulate function and linearly interpolate result
        * introduces interpolation error
        * error is usually proportional to table size
        * if evaluating the table A LOT, want table to be small enough to fit into cache
    * In some cases, can accept interp err b/c we do not know physical value to double precision accuracy (~15-digit)
* Compiler optimization flags
    * From Lecture 4

### More advanced techniques

* memory blocking (Tiling)
    * improve computational intensity ($q$) and temporal locality of data
        * Example: *Strassen* recursive Mat-Mat multiply
* Cache oblivious ordering
    * typically implemented as recursive algorithms and recursive data structures
    * Tiled algorithm requires finding good block size (will depend on hardware)
    * Cache Oblivious Algo offer an alternative
        * idea is to order things in memory to minimize latency with multiple levels of memory hierarchy.
        * Make use of *space filling curves*
* communication avoiding algorithms (State of the Art)
    


