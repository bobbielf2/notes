# Lecture 12, Serial and Parallel Architectures and Performance


### Idealized Processor Model

* Processor names bytes, words, etc. in its address space
    * represent integers, floats, pointers, arrays, etc.
* Operations include
* Order spcified by program
* Idealized Cost
    * Each operation has roughly the same cost (read, write, add, multiply, etc.)

### Real World Processors

* memory controller, register, cache
* much more complicated than the ideal one

### "single" processor Concept

* Real world processors have
    * Registers and caches
    * parallelism
    * pipelining

### Cache

* **Cache** is fast (expensive) memory
* **Cache hit**: in-cache memoroy access - cheap
* **Cache miss**: non-cached memory access - expensive
* **Cache line length**: # of bytes loaded together in one entry
* **Associativity**
    * Direct-mapped
    * another one

### Why multiple levels of cache/memory?

* Most programs have a high degree of *locality* in their memory access patterns
    * Spatial locality
    * temporal locality
    * A *memory hierarchy* attempts to exploit locality to improve overall average access time
* Cache is small and fast (speed = $$$)
* Attempts to reconcile **processor/memory speed gap**

### Memory Hierarchy

Size from small to big, speed from fast to slow: 

- Register 
- L1 
- L2 
- L3 
- DRAM 
- Disk 
- Tape

### Techniques for exposing details of Memory

* Determine memory access times experimentally using "micro"-benchmarks
    * Membench (Saavedra-Barrera), STREAM, others ...
* Membench

#### Access time v.s. Stride (Membench results)

L1: 16 KB, 2 cycles (6ns)  
L2: 2 MB, 12 cycles (36ns)  
Mem: 396 ns (132 cycles)

#### STREAM Benchmark

* Sustainable Memory Bandwidth in High Performance Computers
* Developing 2nd generation benchmark (STREAM2)

## Parallelism

### Single core parallelism

* Pipelining
    * *Dave Patterson's Laundry example*: 4 people doing laundry
* Vector Instructions (SIMD)
    * Single Instruction Multiple Data (SIMD)
    * One operation produces multiple results
    * Implemented as SSE assembly instructions by compiler
        * SSE = Streaming SIMD Insstructions
        * Several standards SSE (128 bit), SSE2,..., SSE4, AVX (256-bit), AVX2 (512-bit)
    * Operate on anything that fits into x bytes
    * Challenges:
        * Need contiguous and aligned data
        * Some instructions are needed to move data around from one register to anther

### Cores, Processors, and Nodes

* Many processors are "multi-core"
* Common for motherboards to have multiple "sockets" or processors
* A node has one motherboard, with multiple processors, and each processor has mulitple cores
* Other terminology:
    * Symmetric Multi-processor (SMP)
    * Non-Uniform Memory Access (NUMA)

#### types of Parallelism

* **Intra-Node** Parallelism (e.g. Multi-core parallelism)    
    * Typically **shared memory parallelism**
    * Common programming models
        * pthreads (POSIX)
        * OpenMP
* **Inter-Node** Parallelism (e.g. Cluster parallelism)    
    * Cluster of nodes is typically
        * 100's to 10000's nodes
        * a network (n-D torus, n-D hypercube, dragonfly)
        * compute nodes are *not* necesarily identical
    * Typically **distributed memory parallelism**
    * Common programming models
        * MPI
        * Unified Parallel C (UPC)

### Comtemporary HPC architecture

* Metadata Servers  
* Scratch Space (Lustre File system) (Umich uses this)  
* Data Transfer Nodes and Login Nodes (for users)  
* Compute Nodes